import streamlit as st
from tools import model, utils
from datetime import datetime
from . import dialog, openai, bing, imagegen, asr
import logging
import re, ast, time

Task = model.Task
Role = model.Role
Message = model.AppMessage

gpt_media_types = openai.accepted_attachment_types
asr_media_types = asr.accepted_types


task_params = {
    model.Task.ChatSearch.value: openai.task_params,
    model.Task.ChatGPT.value: openai.task_params,
    model.Task.GPT4.value: openai.task_params,
    model.Task.GPT4V.value: openai.task_params,
    model.Task.text2img.value: imagegen.task_params,
    model.Task.ASR.value: asr.task_params,
    model.Task.BingAI.value: bing.task_params,
}

## display assistant message
def show_streaming_message(message: Message, message_placeholder):
    i = st.session_state.conversation.index(message)
    role, content, medias =  message.role, message.content, message.medias
    status_placeholder = message_placeholder.empty()
    text_placeholder = message_placeholder.empty()
    if message.queue is not None:
        status_container = None
        while (queue := message.queue) is not None:  # streaming
            while queue.qsize() > 0:
                char = queue.get()
                message.time = datetime.now()
                if isinstance(char, str):
                    if char == model.FINISH_TOKEN:
                        finish_reply(message)
                        break
                    message.content += char
                elif isinstance(char, dict):  # network error
                    if v := char.get(model.SERVER_ERROR):
                        message.content += f'\n\n{v}'
                        message.actions = {'é‡è¯•': model.RETRY_TOKEN}
                        finish_reply(message)
                    elif functions := char.get(model.TOOL_RESULT):
                        # message.content += f'```{json.dumps(v, indent=2, ensure_ascii=False)}```'
                        message.functions += functions
                        # finish_reply(message)
                    elif v := char.get(model.STATUS):
                        if not status_container: #init
                            status_container = status_placeholder.status('æ­£åœ¨æ£€ç´¢', expanded=True)
                        help = char.get(model.HELP)
                        status_container.markdown(v, help=help)
                        message.status.append(v)
                else:
                    raise Exception(f'Unknown content type: {type(content)}')
            # è¶…æ—¶
            if (datetime.now() - message.time).total_seconds() > model.TIMEOUT:
                message.content += '\n\nè¯·æ±‚è¶…æ—¶ï¼Œè¯·é‡è¯•...'
                message.actions = {'é‡è¯•': model.RETRY_TOKEN}
                finish_reply(message)
                break
            # æ¸²æŸ“
            content_full = message.content.replace(model.SUGGESTION_TOKEN, '')
            text_placeholder.markdown(content_full + "â–Œ")
            time.sleep(0.1)
        # remove msg and status
        text_placeholder.empty()
        status_placeholder.empty()
    
    # show non-streaming message
    if message.status: # show status
        with status_placeholder.status('æ­£åœ¨æ£€ç´¢') as status:
            for s in message.status:
                status.write(s)
            status.update(label='æ£€ç´¢å®Œæˆ', state="complete", expanded=False)

    # media
    content = message.content
    suggestions = message.suggestions
    if medias:
        for media in medias:
            display_media(media)
    # suggestion
    if content and (model.SUGGESTION_TOKEN in content or 'å¯å‘æ€§é—®é¢˜:' in content):
        content, suggestions = parse_suggestions(content)
        message.suggestions = suggestions
        message.content = content
    if suggestions and i == len(st.session_state.conversation) - 1:
        suggestions = set(suggestions)
        for suggestion in suggestions:
            message_placeholder.button('ğŸ‘‰ğŸ»'+suggestion[:30], help=suggestion,
                        on_click=gen_response, kwargs={'query': suggestion})
    # text content
    if content:
        text_placeholder.markdown(content)
    # actions: only "retry" is supported
    if (actions := message.actions) and i == len(st.session_state.conversation) - 1:
        for action, token in actions.items():
            message_placeholder.button(action, on_click=handle_action, args=(token,))

## å¯¹è¾“å…¥è¿›è¡Œåº”ç­”
def gen_response(query=None):
    task = st.session_state.task
    assert task in Task.values(), NotImplementedError(task)
    # remove suggestion
    if st.session_state.conversation:
        st.session_state.conversation[-1].suggestions = None
        st.session_state.conversation[-1].actions = None
        
    # get task and input
    user_input = query or st.session_state.input_text
    attachment = st.session_state.get('attachment')
    if not user_input and attachment:
        user_input = attachment.name
    if not user_input:
        return
    
    # create user query
    query_message = Message(
        role = model.Role.user.name,
        name = st.session_state.name, 
        content = user_input, 
        task = model.Task(task).name, 
        time = datetime.now(),
        medias = attachment
    )
    # display and update db
    st.session_state.conversation.append(query_message)
    dialog.update_conversation(st.session_state.name, st.session_state.selected_title, query_message)

    # response
    print(f'Start task({task}): {st.session_state.conversation[-1].content}')
    if task in [Task.ChatGPT.value, Task.GPT4.value, Task.GPT4V.value]:
        queue = openai.chat_stream(conversation=st.session_state.conversation, 
                                    task=task,
                                    attachment=attachment,
                                    guest=st.session_state.guest)
        if openai.DEBUG:
            queue.put('controller: queue returned\n\n')
        bot_response = Message(
            role= Role.assistant.name,
            content = '', 
            queue = queue,
            time = datetime.now(),
            task = model.Task(task).name,
            name = task_params[task][task]['model'],
        )
        st.session_state.conversation.append(bot_response)
    elif task == Task.ChatSearch.value:
        logging.info(f'chat search: {user_input}')
        queue = openai.chat_with_search(conversation=st.session_state.conversation, task=task)
        bot_response = Message(
            role= Role.assistant.name,
            content = '', 
            queue = queue,
            time = datetime.now(),
            task = model.Task(task).name,
            name = task_params[task][task]['model'],
        )
        st.session_state.conversation.append(bot_response)
    elif task == Task.BingAI.value:
        if 'bing' not in st.session_state:
            logging.warning('Initiating BingAI, please wait...')
            # show loading
            st.session_state.bing = bing.BingAI(name=st.session_state.name)
        queue, thread = st.session_state.bing.chat_stream(user_input)
        bot_response = Message(
            role= Role.assistant.name,
            content = '', 
            queue = queue, 
            thread = thread,
            time = datetime.now(),
            name = task_params[task][task]['model'],
        )
        st.session_state.conversation.append(bot_response)
    elif task == Task.text2img.value:
        toast = st.toast('æ­£åœ¨ç»˜åˆ¶', icon='ğŸ–Œï¸')
        with st.spinner('æ­£åœ¨ç»˜åˆ¶'):
            urls = imagegen.gen_image(user_input)
            bot_response = Message(
                role= Role.assistant.name,
                content = None ,
                task = model.Task(task),
                name = task_params[task][task]['model'],
                time = datetime.now(),
                medias = urls
            )
            toast.toast('ç»˜åˆ¶å®Œæˆï¼Œæ­£åœ¨ä¸‹è½½', icon='ğŸ¤©')
            st.session_state.conversation.append(bot_response)
        toast.toast('ä¸‹è½½å®Œæˆ', icon='ğŸ˜„')
        finish_reply(bot_response)
    elif task == Task.ASR.value:
        with st.spinner('æ­£åœ¨è¯†åˆ«'):
            transcription = asr.transcript(attachment)
            bot_response = Message(
                role= Role.assistant.name,
                content = transcription,
                task = model.Task(task),
                name = task_params[task][task]['model'],
                time = datetime.now()
            )
            st.session_state.conversation.append(bot_response)
            finish_reply(bot_response)
    else:
        raise NotImplementedError(task)
    
    return query_message, bot_response


def handle_action(action_token):
    if action_token == model.RETRY_TOKEN:
        bot_response = st.session_state.conversation.pop(-1)
        user_prompt = st.session_state.conversation.pop(-1)
        if bot_response.role == Role.assistant.name and user_prompt.role == Role.user.name:
            user_input = user_prompt.content
            gen_response(query=user_input)
    else:
        raise NotImplementedError(action_token)
    
    
def finish_reply(message):
    message.queue = None
    # if message.thread: # terminate streaming thread
    #     message.thread.join()
    #     message.thread = None
    # else:
    #     logging.info(f'{message.name}: {message.content}')
    dialog.update_conversation(st.session_state.name, st.session_state.selected_title, message)
    print('-'*50)
    

def display_media(media):
    media_type = media.type.split('/')[0]
    if media.type in gpt_media_types or media_type == model.MediaType.image.name:
        media_type == model.MediaType.image
        st.image(media, use_column_width='always')
    elif media.type in asr_media_types or media_type == model.MediaType.audio.name:
        media_type == model.MediaType.audio
        st.audio(media)
    elif media.type == 'mp4' or media_type == model.MediaType.video.name:
        media_type == model.MediaType.video
        st.video(media)
    else:
        raise NotImplementedError(media.tpye)
    return media_type


def call_functions(message):
    functions = message.functions
    queue = message.queue
    
    
## å¤„ç†æç¤º
def parse_suggestions(content:str):
    if not content:
        return None, None
    reply = content
    suggestions = []
    if model.SUGGESTION_TOKEN in content or 'å¯å‘æ€§é—®é¢˜:' in content:
        pattern1 = r'(\[SUGGESTION\]:\s?)(\[.+\])'
        pattern2 = r'(\[SUGGESTION\]:\s?)(.{3,})'
        pattern3 = r'\[SUGGESTION\]|å¯å‘æ€§é—®é¢˜:\s*'
        pattern31 = r'(-\s|\d\.\s)(.+)'
        matches1 = re.findall(pattern1, reply)
        matches2 = re.findall(pattern2, reply)
        matches3 = re.findall(pattern3, reply)
        
        if matches1:
            for m in matches1:
                reply = reply.replace(''.join(m), '')
                try:
                    suggestions += ast.literal_eval(m[1])
                except:
                    print('==>Error parsing suggestion:<===\n', content)
        elif len(matches2)>=3:
            for m in matches2:
                reply = reply.replace(''.join(m), '')
                suggestions.append(m[1].strip())
        elif matches3:
            # assume only one match
            replies = content.split(matches3[0])
            reply = replies[0]
            for r in replies[1:]:
                match31 = re.findall(pattern31, r)
                suggestions += [m[1].strip() for m in match31]
                for m in match31:
                    r = r.replace(''.join(m), '')
                reply += r

    return reply, suggestions

def filter_suggestion(content:str):
    pattern = r'\[?SUGGESTION\].*$'
    content = '\n'.join(re.split(pattern, content, re.MULTILINE))
    return content



if __name__ == '__main__':
    content = '''ä½œä¸ºåŸºäºTransformeræŠ€æœ¯çš„AIåŠ©æ‰‹ï¼Œæˆ‘æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š

1. è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼šæˆ‘å¯ä»¥ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼Œå¹¶é€šè¿‡è¯­ä¹‰ç†è§£å’Œè¯­è¨€ç”ŸæˆæŠ€æœ¯æ¥å›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œè¿›è¡Œå¯¹è¯ã€‚

2. çŸ¥è¯†æ£€ç´¢å’Œæ¨ç†ï¼šæˆ‘å¯ä»¥ä»å¹¿æ³›çš„çŸ¥è¯†åº“ä¸­æ£€ç´¢å’Œæå–ä¿¡æ¯ï¼ŒåŒ…æ‹¬äº‹å®ã€å®šä¹‰ã€è§£é‡Šã€ç»Ÿè®¡æ•°æ®ç­‰ï¼Œå¹¶è¿›è¡Œæ¨ç†å’Œé€»è¾‘æ¨æ–­ã€‚

3. é—®é¢˜è§£ç­”å’Œå’¨è¯¢ï¼šæˆ‘å¯ä»¥å›ç­”å„ç§AIç›¸å…³çš„é—®é¢˜ï¼ŒåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸï¼Œå¹¶æä¾›å’¨è¯¢å’Œå»ºè®®ã€‚

4. æ•°æ®æ ‡æ³¨å’Œæ•°æ®ç­–ç•¥ï¼šä½œä¸ºæ˜Ÿå°˜æ•°æ®çš„AIåŠ©æ‰‹ï¼Œæˆ‘å¯ä»¥å¸®åŠ©è§£ç­”ä¸æ•°æ®æ ‡æ³¨å’Œæ•°æ®ç­–ç•¥ç›¸å…³çš„é—®é¢˜ï¼ŒåŒ…æ‹¬æ•°æ®é›†çš„æ„å»ºã€æ ‡æ³¨è´¨é‡æ§åˆ¶ã€æ ‡æ³¨å·¥å…·é€‰æ‹©ç­‰æ–¹é¢ã€‚

5. æä¾›ç›¸å…³èµ„æºå’ŒæŒ‡å¯¼ï¼šå¦‚æœæˆ‘æ— æ³•å›ç­”ä½ çš„é—®é¢˜ï¼Œæˆ‘ä¼šå»ºè®®ä½ è®¿é—®æ˜Ÿå°˜æ•°æ®çš„å®˜æ–¹ç½‘ç«™ï¼ˆstardust.aiï¼‰ï¼Œé‚£é‡Œä¼šæœ‰æ›´å¤šå…³äºAIå’Œæ•°æ®é¢†åŸŸçš„èµ„æºå’ŒæŒ‡å¯¼ã€‚

å¯å‘æ€§é—®é¢˜:
- ä½ èƒ½ç»™æˆ‘æä¾›ä¸€äº›å…³äºè‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨é¢†åŸŸå—ï¼Ÿ
- åœ¨æ•°æ®æ ‡æ³¨è¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•ç¡®ä¿æ ‡æ³¨è´¨é‡ï¼Ÿ
- ä½ èƒ½å‘æˆ‘è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ æ˜¯å¦‚ä½•å·¥ä½œçš„å—ï¼Ÿ'''
    content, suggestion = parse_suggestions(content)
    print(content)
